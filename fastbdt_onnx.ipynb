{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9a99b4-f10b-4fff-b6a7-9da80f8d8fad",
   "metadata": {},
   "source": [
    "Run notebook with\n",
    "\n",
    "```bash\n",
    "uv run --with uv --with jupyter jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538e1d75-ac5f-4f7c-9bed-878bcf447d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install onnx onnxruntime onnxmltools skl2onnx scikit-learn xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77f6e7f-a65f-4384-9122-2d5f39fb033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import base64\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446c165-bf92-47d1-a25f-b68c5e923c86",
   "metadata": {},
   "source": [
    "## Parse FastBDT weights\n",
    "\n",
    "Looking into\n",
    "- https://github.com/thomaskeck/FastBDT/blob/master/src/Classifier.cxx#L209\n",
    "- https://github.com/thomaskeck/FastBDT/blob/master/src/FastBDT_IO.cxx\n",
    "- https://github.com/thomaskeck/FastBDT/blob/master/include/FastBDT_IO.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14af6b6c-ccec-4678-aea9-c48d42dcef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/nikolai/code/basf2/mva/methods/tests/FastBDTv5.xml\") as f:\n",
    "    data = base64.b64decode(xmltodict.parse(f\"<root>{f.read().split('?>')[1]}</root>\")[\"root\"][\"FastBDT_Weightfile\"]+\"===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f6a54-b7bc-42ad-ab61-620425e22289",
   "metadata": {},
   "source": [
    "Data is read from space separated numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acda5fc4-e81a-4bbf-83ef-847265a4c80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[b'1', b'200', b'3', b'3', b'8', b'8', b'8', b'0.1', b'1', b'0', b'-1', b'3', b'0', b'0', b'0', b'1', b'3', b'8', b'257', b'1.700299740e+00', b'1.847970366e+00', b'1.823094368e+00', b'1.871596694e+00', b'1.810523272e+00', b'1.835662246e+00', b'1.859904289e+00', b'1.884671807e+00', b'1.803922415e+00', b'1.816829801e+00', b'1.829383850e+00', b'1.841796398e+00', b'1.854024768e+00', b'1.865672946e+00', b'1.877931833e+00', b'1.891820669e+00', b'1.800248265e+00', b'1.807294846e+00', b'1.813672304e+00', b'1.819913387e+00', b'1.826185822e+00', b'1.832525253e+00', b'1.838704467e+00', b'1.844905376e+00', b'1.850979686e+00', b'1.857039809e+00', b'1.862837672e+00', b'1.868579149e+00', b'1.874693990e+00', b'1.881253123e+00', b'1.888190508e+00']\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = data.split()\n",
    "str(tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34d6029f-9931-4984-b447-fe555896c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d62ad7f-d299-4312-abef-ab8f70116860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(tokens, conv=int):\n",
    "    logger.debug(f\"read {conv}\")\n",
    "    return conv(next(tokens))\n",
    "\n",
    "def read_vector(tokens, conv=float):\n",
    "    logger.debug(f\"read vector<{conv}>\")\n",
    "    size = int(next(tokens))\n",
    "    return [conv(next(tokens)) for i in range(size)]\n",
    "\n",
    "def read_vector_feature_binning(tokens):\n",
    "    logger.debug(f\"read vector of feature binning\")\n",
    "    out = []\n",
    "    size = read(tokens, int)\n",
    "    for i in range(size):\n",
    "        n_levels = read(tokens, int)\n",
    "        binning = read_vector(tokens, float)\n",
    "        out.append((n_levels, binning))\n",
    "    return out\n",
    "\n",
    "@dataclass\n",
    "class Cut:\n",
    "    feature: int\n",
    "    index: ...\n",
    "    gain: float\n",
    "    valid: int\n",
    "\n",
    "    @classmethod\n",
    "    def from_tokens(cls, tokens, conv=float):\n",
    "        logger.debug(f\"Read Cut<{conv}>\")\n",
    "        feature = read(tokens, int)\n",
    "        index = read(tokens, conv)\n",
    "        valid = read(tokens, int)\n",
    "        gain = read(tokens, float)\n",
    "        return cls(feature, index, gain, valid)\n",
    "\n",
    "@dataclass\n",
    "class Tree:\n",
    "    cuts: list[Cut]\n",
    "    nEntries: int\n",
    "    purities: float\n",
    "    boost_weights: list[float]\n",
    "\n",
    "    @classmethod\n",
    "    def from_tokens(cls, tokens, conv=float):\n",
    "        logger.debug(f\"Read Tree<{conv}>\")\n",
    "        size = read(tokens, int)\n",
    "        cuts = []\n",
    "        for i in range(size):\n",
    "            cuts.append(Cut.from_tokens(tokens, conv))\n",
    "        boost_weights = read_vector(tokens, float)\n",
    "        purities = read_vector(tokens, float)\n",
    "        nEntries = read_vector(tokens, float)\n",
    "        return cls(cuts, nEntries, purities, boost_weights)\n",
    "\n",
    "@dataclass\n",
    "class Forest:\n",
    "    f0: float\n",
    "    shrinkage: float\n",
    "    transform2probability: list[bool]\n",
    "    trees: list[Tree]\n",
    "\n",
    "    @classmethod\n",
    "    def from_tokens(cls, tokens, conv=float):\n",
    "        logger.debug(f\"Read Forest<{conv}>\")\n",
    "        f0 = read(tokens, float)\n",
    "        shrinkage = read(tokens, float)\n",
    "        transform2probability = read(tokens, bool)\n",
    "        size = read(tokens, int)\n",
    "        trees = []\n",
    "        for i in range(size):\n",
    "            trees.append(Tree.from_tokens(tokens, conv))\n",
    "        return cls(f0, shrinkage, transform2probability, trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71914b13-693b-4366-b80b-6894483cf45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 8, 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_vector(iter(data.split()[3:]), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbddbcc0-dee4-4ba8-b2d5-e207e11924db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debugging_iter(it):\n",
    "    for x in it:\n",
    "        print(f\"Reading {x}\")\n",
    "        yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e20d9b6-a107-4cbe-a2c0-70df01cfc0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BDT:\n",
    "    version: int\n",
    "    n_trees: int\n",
    "    depth: int\n",
    "    binning: list[int]\n",
    "    shrinkage: float\n",
    "    subsample: float\n",
    "    sPlot: bool\n",
    "    flatnessLoss: float\n",
    "    purityTransformation: list[bool]\n",
    "    transform2probability: bool\n",
    "    featureBinning: list[tuple[int, list[float]]]\n",
    "    purityBinning: list[int]\n",
    "    numberOfFeatures: int\n",
    "    numberOfFinalFeatures: int\n",
    "    numberOfFlatnessFeatures: int\n",
    "    can_use_fast_forest: bool\n",
    "    forest: Forest\n",
    "    binned_forest: Forest\n",
    "\n",
    "    @classmethod\n",
    "    def from_tokens(cls, tokens):\n",
    "        return cls(\n",
    "            version=read(tokens, int),\n",
    "            n_trees=read(tokens, int),\n",
    "            depth=read(tokens, int),\n",
    "            binning=read_vector(tokens, int),\n",
    "            shrinkage=read(tokens, float),\n",
    "            subsample=read(tokens, float),\n",
    "            sPlot=read(tokens, bool),\n",
    "            flatnessLoss=read(tokens, float),\n",
    "            purityTransformation=read_vector(tokens, bool),\n",
    "            transform2probability=read(tokens, bool),\n",
    "            featureBinning=read_vector_feature_binning(tokens),\n",
    "            purityBinning=read_vector(tokens, int),\n",
    "            numberOfFeatures=read(tokens, int),\n",
    "            numberOfFinalFeatures=read(tokens, int),\n",
    "            numberOfFlatnessFeatures=read(tokens, int),\n",
    "            can_use_fast_forest=read(tokens, bool),\n",
    "            forest=Forest.from_tokens(tokens, float),\n",
    "            binned_forest=Forest.from_tokens(tokens, int),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d0dad02-12b9-4fb5-8d2a-4d294547e0a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bdt = BDT.from_tokens(iter(data.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "013b68cf-df1c-49a4-8c8f-100a69e45336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['version', 'n_trees', 'depth', 'binning', 'shrinkage', 'subsample', 'sPlot', 'flatnessLoss', 'purityTransformation', 'transform2probability', 'featureBinning', 'purityBinning', 'numberOfFeatures', 'numberOfFinalFeatures', 'numberOfFlatnessFeatures', 'can_use_fast_forest', 'forest', 'binned_forest'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb426adb-1512-4eaa-a2b2-3f462f41e470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bdt.forest.trees[0].cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "236fc435-feb0-4dba-a5dc-63d419fb1faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree(cuts=[Cut(feature=0, index=1.836806893, gain=1062.871, valid=1), Cut(feature=0, index=1.820293784, gain=67.60223, valid=1), Cut(feature=2, index=1.23523581, gain=153.043, valid=1), Cut(feature=0, index=1.807693601, gain=7.638123, valid=1), Cut(feature=0, index=1.831332564, gain=12.39661, valid=1), Cut(feature=0, index=1.886921883, gain=98.42749, valid=1), Cut(feature=0, index=1.882526875, gain=43.83337, valid=1)], nEntries=[36198.39062, 9302.176758, 26905.32422, 4896.272461, 4401.743164, 19258.10352, 7651.693848, 2047.376343, 2849.948242, 2790.36084, 1611.965332, 16794.15039, 2463.5271, 6486.14502, 1165.553955], purities=[0.5001226068, 0.208625108, 0.6007550359, 0.1277961731, 0.2987351418, 0.553139925, 0.7201112509, 0.0811952427, 0.1612261832, 0.2583843768, 0.3684764504, 0.5804541707, 0.3669680953, 0.7521873713, 0.5416552424], boost_weights=[0.0002458892995, -0.5826275945, 0.2019119263, -0.7445226312, -0.4025033712, 0.1062627062, 0.4403621256, -0.8376281857, -0.6774736047, -0.4831543863, -0.2630733252, 0.1607446969, -0.2660554945, 0.5045183301, 0.08330724388])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.forest.trees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c444652-a707-4d22-adde-44ce4ee3571b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.numberOfFeatures, bdt.numberOfFinalFeatures, bdt.numberOfFlatnessFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a3905-f802-4fa8-970e-0c6f57d48016",
   "metadata": {},
   "source": [
    "## Learn how onnx does BDTs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e72afe-9215-4229-ba02-9f10c8a27465",
   "metadata": {},
   "source": [
    "see https://onnx.ai/onnx/operators/onnx_aionnxml_TreeEnsemble.html\n",
    "\n",
    "Note: seems the conversion tools for sklearn and xgboost still (2025-09) use the \"deprecated\" `TreeEnsembleClassifier` nodes - so let's try to use the new `TreeEnsemble` here:\n",
    "\n",
    "(see also https://github.com/onnx/onnx/pull/5874)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc9e9c4d-55c6-4fdb-acb7-302a0f51d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx.backend.test.case.node import expect, _extract_value_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96cc62e3-3420-414e-a3a9-8fc3cced57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx.helper import make_tensor, make_value_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5064003-491b-44a5-9d84-ce6437cbe3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = onnx.helper.make_node(\n",
    "    \"TreeEnsemble\",\n",
    "    [\"input\"],\n",
    "    [\"output\"],\n",
    "    domain=\"ai.onnx.ml\",\n",
    "    n_targets=2,\n",
    "    membership_values=None,\n",
    "    nodes_missing_value_tracks_true=None,\n",
    "    nodes_hitrates=None,\n",
    "    aggregate_function=1,\n",
    "    post_transform=0,\n",
    "    tree_roots=[0],\n",
    "    nodes_modes=make_tensor(\n",
    "        \"nodes_modes\",\n",
    "        onnx.TensorProto.UINT8,\n",
    "        (3,),\n",
    "        np.array([0, 0, 0], dtype=np.uint8),\n",
    "    ),\n",
    "    nodes_featureids=[0, 0, 0],\n",
    "    nodes_splits=make_tensor(\n",
    "        \"nodes_splits\",\n",
    "        onnx.TensorProto.DOUBLE,\n",
    "        (3,),\n",
    "        np.array([3.14, 1.2, 4.2], dtype=np.float64),\n",
    "    ),\n",
    "    nodes_truenodeids=[1, 0, 1],\n",
    "    nodes_trueleafs=[0, 1, 1],\n",
    "    nodes_falsenodeids=[2, 2, 3],\n",
    "    nodes_falseleafs=[0, 1, 1],\n",
    "    leaf_targetids=[0, 1, 0, 1],\n",
    "    leaf_weights=make_tensor(\n",
    "        \"leaf_weights\",\n",
    "        onnx.TensorProto.DOUBLE,\n",
    "        (4,),\n",
    "        np.array([5.23, 12.12, -12.23, 7.21], dtype=np.float64),\n",
    "    ),\n",
    ")\n",
    "\n",
    "x = np.array([1.2, 3.4, -0.12, 1.66, 4.14, 1.77], np.float64).reshape(3, 2)\n",
    "y = np.array([[5.23, 0], [5.23, 0], [0, 12.12]], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9883289b-18fa-423e-8478-5171c2f91de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following loosely https://onnx.ai/onnx/intro/python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba954055-0d1a-4a97-9169-45c1b232a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import TensorProto\n",
    "from onnx.helper import (\n",
    "    make_model, make_node, make_graph,\n",
    "    make_tensor_value_info)\n",
    "from onnx.checker import check_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "defb6553-838f-4b05-9529-0059d955bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = make_graph(\n",
    "    [node],\n",
    "    \"bla\",\n",
    "    [make_tensor_value_info(\"input\", TensorProto.DOUBLE, [3, 2])],\n",
    "    [make_tensor_value_info(\"output\", TensorProto.DOUBLE, [3, 2])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5874a24a-b83d-4f84-8bee-1e450c0bdb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(graph, opset_imports=[onnx.helper.make_opsetid(\"ai.onnx.ml\", 5)], ir_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72a75df3-4693-4284-957a-d677a2d70421",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14bf580e-5621-4be3-928a-3e2c0f648ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "244115b8-6c7f-4960-93d4-a89304141722",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = onnxruntime.InferenceSession(model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aab52c77-3dac-4811-800c-40c7da4978f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[5.23, 0.  ],\n",
       "        [5.23, 0.  ],\n",
       "        [5.23, 0.  ]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([\"output\"], {\"input\": np.random.rand(3, 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b187b940-a51d-4c49-841b-4f92a11449b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (sess.run([\"output\"], {\"input\": x}) == y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b2c04-c882-49a2-8545-d3cfbd2ad62c",
   "metadata": {},
   "source": [
    "## Now convert the FastBDT stuff into what onnx needs\n",
    "\n",
    "Have to flatten all trees into single lists for nodes and leafs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19209ada-e5cd-4a3c-b36e-44f745df2c4b",
   "metadata": {},
   "source": [
    "FastBDT uses trees with fixed depth, so `2**depth` is the number of leafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf130142-1e2f-4036-96ef-2a1d8263059d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_leafs = 2**(bdt.depth)\n",
    "n_leafs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4bee7-4dd8-4184-8fd1-3f5999f48867",
   "metadata": {},
   "source": [
    "We have only one target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "643c5d3a-c9e5-4804-a35e-3f94ab3cf868",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_targetids = [0] * len(bdt.forest.trees) * n_leafs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc102aa6-ba98-4032-8715-3a81d8ddeb20",
   "metadata": {},
   "source": [
    "last `n_leafs` entries are `boost_weights` for leafs\n",
    "\n",
    "**TODO:** handle the \"invalid\" cuts - this is a bit ugly since we need to add extra leafs for these (FastBDT seems to treat these as terminal nodes and there will be a unique boost weight assigned no matter the value - so maybe we could reference both false and true branch to the same leaf in these cases. If we actually keep the index value `NaN` it will always be sent to the false branch probably.)\n",
    "\n",
    "**TODO:** Treatment of nan **inputs** is even more annoying. I'm still not quite sure i have fully understood the behavior of FastBDT to \"stop\" when a NaN value is seen. As far as i currently understand there is then a possible leaf for every internal node where it might stop. Maybe the only way this could be implemented is to add for *every* node an additional node that decides the leaf in case of nan input (there will be one possible leaf for every original node as far as i understood). The comparison value of these nodes would be NaN such that they evaluate to false for every value, but then set `nodes_missing_value_tracks_true` to 1, such that for NaN input it will evaluate to true and send them to the special leafs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7b25b07-ca00-47d9-a6a6-426f58441bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_weights = [item * bdt.shrinkage for tree in bdt.forest.trees for item in tree.boost_weights[-n_leafs:]]\n",
    "leaf_weights = make_tensor(\n",
    "    \"leaf_weights\",\n",
    "    onnx.TensorProto.DOUBLE,\n",
    "    (len(leaf_weights),),\n",
    "    np.array(leaf_weights, dtype=np.float64)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd34bcb-57f5-4a4a-a4d6-2de882d92224",
   "metadata": {},
   "source": [
    "> * **nodes_falseleafs - INTS** (required): \n",
    "> 1 if false branch is leaf for each node and 0 if an interior node. To represent a tree that is a leaf (only has one node), one can do so by having a    single nodes_* entry with true and false branches referencing the same leaf_* entry\n",
    "\n",
    "The last `n_leafs // 2` nodes should be the terminal nodes, where both false and true branches are leafs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c66c462b-1af4-43fb-b9e0-1e75e7760647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 3, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_nodes = len(bdt.forest.trees[0].cuts)\n",
    "n_terminal_nodes = n_leafs // 2\n",
    "n_internal_nodes = n_nodes - n_terminal_nodes\n",
    "n_nodes, n_internal_nodes, n_terminal_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0287f2f-254d-479a-ab45-a8fb8c5fc80a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodes_falseleafs = ([0] * n_internal_nodes + [1] * (n_leafs // 2)) * len(bdt.forest.trees)\n",
    "nodes_trueleafs = list(nodes_falseleafs)\n",
    "#nodes_falseleafs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a00fb0-f48c-4343-84fc-5c57ebf6d82d",
   "metadata": {},
   "source": [
    "> **nodes_falsenodeids - INTS** (required) :\n",
    "> If `nodes_falseleafs` is false at an entry, this represents the position of the false branch node. This position can be used to index into a `nodes_*` entry. If `nodes_falseleafs` is false, it is an index into the `leaf_*` attributes.\n",
    "\n",
    "Excerpt from FastBDT code:\n",
    "\n",
    "```c++\n",
    "            // Perform the cut of the given node and update the node.\n",
    "            // Either the event is passed to the left child node (which has\n",
    "            // the position 2*node in the next layer) or to the right\n",
    "            // (which has the position 2*node + 1 in the next layer)\n",
    "            node = (node << 1) + static_cast<unsigned int>(value >= cut.index);\n",
    "```\n",
    "\n",
    "So we will use all cut indices such that if `value >= cut.index` (let's call this \"true\") we go to node `2*node + 1`, `2*node` otherwise.\n",
    "\n",
    "Here we will have global indices, so need to count offsets up while looping through the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cd2b055-730c-42ee-833b-9ab9530c1953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, [1, 2, 3], [4, 5, 6, 7], 7)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_internal_nodes, list(range(1, n_internal_nodes + 1)), list(range(n_internal_nodes + 1, n_nodes + 1)), n_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcfe3e9b-afdd-4e9d-a8e3-14552e6a45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_falsenodeids = []\n",
    "nodes_truenodeids = []\n",
    "node_offset = 0\n",
    "i_leaf = 0\n",
    "for tree in bdt.forest.trees:\n",
    "    for node in range(1, n_internal_nodes + 1): # iterate with 1-based indices, but need to fill in 0-based ones\n",
    "        nodes_falsenodeids.append(2 * node + node_offset - 1)\n",
    "        nodes_truenodeids.append(2 * node + 1 + node_offset - 1)\n",
    "    for node in range(n_internal_nodes + 1, n_nodes + 1):\n",
    "        nodes_falsenodeids.append(i_leaf); i_leaf += 1\n",
    "        nodes_truenodeids.append(i_leaf); i_leaf += 1\n",
    "    node_offset += len(tree.cuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ebe8b9-fef3-425c-96e7-ae36037acb31",
   "metadata": {},
   "source": [
    "`nodes_featureids` is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b6b3af8-58d8-4c56-a3a4-0819bc99da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_featureids = []\n",
    "for tree in bdt.forest.trees:\n",
    "    for cut in tree.cuts:\n",
    "        nodes_featureids.append(cut.feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca588a-c7e6-452f-a079-0370a7b09e72",
   "metadata": {},
   "source": [
    "> * **nodes_modes - TENSOR** (required) :\n",
    "> The comparison operation performed by the node. This is encoded as an enumeration of 0 (‘BRANCH_LEQ’), 1 (‘BRANCH_LT’), 2 (‘BRANCH_GTE’), 3 (‘BRANCH_GT’), 4 (‘BRANCH_EQ’), 5 (‘BRANCH_NEQ’), and 6 (‘BRANCH_MEMBER’). Note this is a tensor of type uint8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d52cf0e-9a04-48be-b3c4-7823fe37d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH_GTE = 3 # i guess it means value >= index ?\n",
    "shape = (len(nodes_featureids),)\n",
    "nodes_modes = make_tensor(\n",
    "    \"nodes_modes\",\n",
    "    onnx.TensorProto.UINT8,\n",
    "    shape,\n",
    "    np.full(shape, BRANCH_GTE, dtype=np.uint8),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f39403-a428-4bf1-9e10-d2a55d4fcb6a",
   "metadata": {},
   "source": [
    "`nodes_splits` is also easy, just the cut indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "433ba768-5672-4293-9210-533ba461d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_splits = []\n",
    "for tree in bdt.forest.trees:\n",
    "    for cut in tree.cuts:\n",
    "        nodes_splits.append(cut.index)\n",
    "nodes_splits = make_tensor(\n",
    "    \"nodes_splits\",\n",
    "    onnx.TensorProto.DOUBLE,\n",
    "    (len(nodes_splits),),\n",
    "    np.array(nodes_splits, dtype=np.float64)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d95bbc-e008-4c4a-83bb-ad6e80b0a4d2",
   "metadata": {},
   "source": [
    "and `tree_roots` as well, first node for every tree is the root node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa4ae46f-91be-4ab4-ac49-f1a8f9b42ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_roots = []\n",
    "i = 0\n",
    "for tree in bdt.forest.trees:\n",
    "    tree_roots.append(i)\n",
    "    i += len(tree.cuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454054f-f95f-4225-9bc6-5f02be227fa4",
   "metadata": {},
   "source": [
    "build the onnx tree ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee8a1aea-8821-4cde-a95d-ddc1149c0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = onnx.helper.make_node(\n",
    "    \"TreeEnsemble\",\n",
    "    [\"input\"],\n",
    "    [\"output\"],\n",
    "    domain=\"ai.onnx.ml\",\n",
    "    n_targets=1,\n",
    "    membership_values=None,\n",
    "    nodes_missing_value_tracks_true=None,\n",
    "    nodes_hitrates=None,\n",
    "    aggregate_function=1,\n",
    "    post_transform=0,\n",
    "    tree_roots=tree_roots,\n",
    "    nodes_modes=nodes_modes,\n",
    "    nodes_featureids=nodes_featureids,\n",
    "    nodes_splits=nodes_splits,\n",
    "    nodes_truenodeids=nodes_truenodeids,\n",
    "    nodes_trueleafs=nodes_trueleafs,\n",
    "    nodes_falsenodeids=nodes_falsenodeids,\n",
    "    nodes_falseleafs=nodes_falseleafs,\n",
    "    leaf_targetids=leaf_targetids,\n",
    "    leaf_weights=leaf_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "567333f3-7187-401c-8093-e649ff647ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = make_graph(\n",
    "    [node],\n",
    "    \"bla\",\n",
    "    [make_tensor_value_info(\"input\", TensorProto.DOUBLE, [1, bdt.numberOfFeatures])],\n",
    "    [make_tensor_value_info(\"output\", TensorProto.DOUBLE, [1, 1])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddade69b-abb1-4065-8967-6f0964601068",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(graph, opset_imports=[onnx.helper.make_opsetid(\"ai.onnx.ml\", 5)], ir_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d870fef7-d792-48e4-a30b-3bf90b827bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8201158d-2f5e-4fb5-975d-f04c040f33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1a34d1d-0ce2-43e6-89ba-af7f99bc9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = onnxruntime.InferenceSession(model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cbcf4d8-277a-4a9a-8440-717e89d74bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.80847116]])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([\"output\"], {\"input\": np.random.rand(1, 3)*3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0162f66-727a-4e45-962a-0e3025521cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0002458892995,\n",
       " -0.5826275945,\n",
       " 0.2019119263,\n",
       " -0.7445226312,\n",
       " -0.4025033712,\n",
       " 0.1062627062,\n",
       " 0.4403621256,\n",
       " -0.8376281857,\n",
       " -0.6774736047,\n",
       " -0.4831543863,\n",
       " -0.2630733252,\n",
       " 0.1607446969,\n",
       " -0.2660554945,\n",
       " 0.5045183301,\n",
       " 0.08330724388]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.forest.trees[0].boost_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0caaf1-e49b-4bff-bbb4-39cf0ebc458e",
   "metadata": {},
   "source": [
    "## Crosscheck with what i think should be happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e296aed3-04b2-40ba-8d42-8faa0463116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(bdt, values):\n",
    "    out = 0\n",
    "    for tree in bdt.forest.trees:\n",
    "        node = 1\n",
    "        while node <= len(tree.cuts):\n",
    "            cut = tree.cuts[node - 1]\n",
    "            value = values[cut.feature]\n",
    "            node = (node << 1) + int(value >= cut.index)\n",
    "        out += tree.boost_weights[node - 1] * bdt.shrinkage\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9f3a85f-656b-4903-a65a-01b7f2f68470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9565154768815716"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = np.random.rand(3) * 2\n",
    "apply(bdt, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7512099-ad7c-4913-a322-935d487c2f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.95651548]])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([\"output\"], {\"input\": np.array([inp])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1bc40b9-e9bb-4fca-87b6-3f2ddd45be7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.836806893"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_splits.double_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51526bf1-76fd-48d1-8e0d-6da7dc2c6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply2(values):\n",
    "    out = 0\n",
    "    for i_root in tree_roots:\n",
    "        node = i_root\n",
    "        while True:\n",
    "            index = nodes_splits.double_data[node]\n",
    "            value = values[nodes_featureids[node]]\n",
    "            if value >= index:\n",
    "                next_node = nodes_truenodeids[node]\n",
    "                if nodes_trueleafs[node]:\n",
    "                    out += leaf_weights.double_data[next_node]\n",
    "                    break\n",
    "            else:\n",
    "                next_node = nodes_falsenodeids[node]\n",
    "                if nodes_falseleafs[node]:\n",
    "                    out += leaf_weights.double_data[next_node]\n",
    "                    break\n",
    "            node = next_node\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0dfc83ea-001b-4165-83d6-f02d34cccc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9565154768815716"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply2(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d49749a-6241-4c86-afb3-5a4641f63a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.onnx\", \"wb\") as f:\n",
    "    onnx.save(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
